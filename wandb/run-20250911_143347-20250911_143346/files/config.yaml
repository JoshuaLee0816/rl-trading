_wandb:
    value:
        cli_version: 0.21.3
        e:
            quvmi8rmki7xgm4zoocdn6vsqvsw7dj9:
                codePath: src\rl\train\ppo_test.py
                codePathLocal: src\rl\train\ppo_test.py
                cpu_count: 10
                cpu_count_logical: 12
                disk:
                    /:
                        total: "509845245952"
                        used: "431896707072"
                email: 310274movie@gmail.com
                executable: C:\Users\user\Desktop\李哲熙\02_財金\RL_Trading\.venv\scripts\python.exe
                git:
                    commit: bd8716c5ee1dabc42bf7f4c83c1c4cadfd21721b
                    remote: https://github.com/JoshuaLee0816/rl-trading.git
                host: LAPTOP-N1U6QE0M
                memory:
                    total: "16856166400"
                os: Windows-11-10.0.26100-SP0
                program: C:\Users\user\Desktop\李哲熙\02_財金\RL_Trading\src\rl\train\ppo_test.py
                python: CPython 3.12.10
                root: C:\Users\user\Desktop\李哲熙\02_財金\RL_Trading
                startedAt: "2025-09-11T06:33:47.719288Z"
                writerId: quvmi8rmki7xgm4zoocdn6vsqvsw7dj9
        m: []
        python_version: 3.12.10
        t:
            "1":
                - 1
            "2":
                - 1
            "3":
                - 13
                - 14
                - 16
            "4": 3.12.10
            "5": 0.21.3
            "8":
                - 3
            "12": 0.21.3
            "13": windows-amd64
data:
    value:
        file: full/walk_forward/WF_train_2015_2019_full.parquet
dqn:
    value:
        batch_size: 64
        buffer_size: 10000
        device: auto
        epsilon_decay: 0.9995
        epsilon_min: 0.05
        epsilon_schedule: per_episode
        epsilon_start: 1
        gamma: 0.99
        lr: 0.0005
        target_update_freq: 100
environment:
    value:
        action_mode: discrete
        initial_cash: 100000
        lookback: 20
        max_holdings: 5
        qmax_per_trade: 10
        reward_mode: daily_return
logging:
    value:
        outdir: logs/runs
        save_summary: false
        save_trades: false
        trade_sample_freq: 1
ppo:
    value:
        actor_hidden: 128
        actor_lr: 0.0003
        batch_size: 64
        clip_epsilon: 0.2
        critic_hidden: 64
        critic_lr: 0.001
        device: cpu
        entropy_coef: 0.05
        epochs: 3
        gae_lambda: 0.95
        gamma: 0.99
        n_steps: 512
        num_envs: 2
        use_subproc: true
        value_coef: 0.5
safety:
    value:
        cool_down_every: 500
        sleep_seconds: 120
training:
    value:
        checkpoint_freq: 100
        grad_steps: 1
        model: ppo
        n_episodes: 2000
        save_freq: 5
        seed: 42
        update_every: 512
        warmup_steps: 10000
