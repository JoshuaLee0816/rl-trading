training:
  model: "ppo"                            #控制哪個model跑模型
  n_episodes: 5000                           # 總訓練集數
  save_freq: 5                             # 每幾集更新一次 reward_curve.png
  seed: 42                                  # 隨機種子，方便重現結果
  model: "dqn"                           # Random, DQN, PPO, A2C...
  update_every: 50                         # 每幾步更新一次
  warmup_steps: 10000                      # 至少先收 10000 步資料再開始更新
  grad_steps: 1                           # 每次觸發時連續更新幾次

data:
  #file: "training_data_20.csv"
  file: "training_data_all.parquet"

safety:
  cool_down_every: 200                      # 每多少 episodes 強制休息
  sleep_seconds: 120                         # 休息幾秒

environment:
  initial_cash: 100000                       # 初始資金(十萬)
  lookback: 20                                # 向前看的天數
  reward_mode: "daily_return"                 # 可選 daily_return / total_profit
  action_mode: "weights"                      # MultiDiscrete 動作模式
  max_holdings: 3                             # 最多同時持有幾檔股票 (目前先做限制備用)

logging:
  outdir: "logs/runs"                         # 輸出目錄（會自動建立 run_XXXX 資料夾）
  save_summary: true                          # 是否存 summary.csv
  save_trades: False                           # 是否存 trades_all.csv
  trade_sample_freq: 1                       # 每幾筆交易存一次（logger 控制）

dqn:
  gamma: 0.99
  epsilon_schedule: "per_episode"      # "per_episode" | "per_step"
  epsilon_start: 1.0
  epsilon_min: 0.05
  epsilon_decay: 0.9995
  lr: 0.0005
  batch_size: 64
  buffer_size: 10000
  target_update_freq: 100              # 每幾次update就把Q Network 參數複製給target network
  device: "auto"                          # 可選 "cpu"、"mps"、"cuda"、"auto"

ppo:
  gamma: 0.99
  gae_lambda: 0.95
  clip_epsilon: 0.2
  actor_lr: 0.0003     
  critic_lr: 0.001     
  batch_size: 256
  n_steps: 2048
  epochs: 10
  entropy_coef: 0.01
  value_coef: 0.5
  device: "auto"
  num_envs: 4                         #多少顆CPU一起收集資料     


